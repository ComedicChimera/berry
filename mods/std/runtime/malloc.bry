module runtime;

/* ------------------------------ Allocator API ----------------------------- */

pub func malloc(size: uint) *u8 {
    let rstate = get_rstate();

    rstate.panic_depth++;
    let data = rstate.heap.malloc(size);
    rstate.panic_depth--;

    return data;
}

pub func mrealloc(data: *u8, new_size: uint) *u8 {
    let rstate = get_rstate();

    rstate.panic_depth++;
    data = rstate.heap.realloc(data, new_size);
    rstate.panic_depth--;

    return data;
}

pub func mfree(data: *u8) {
    let rstate = get_rstate();

    rstate.panic_depth++;
    rstate.heap.mfree(data);
    rstate.panic_depth--;
}

/* -------------------------------- Constants ------------------------------- */

// Useful Size Constants
const KB: uint = 1024;
const MB: uint = KB * KB;
const GB: uint = KB * KB * KB;

// Platform Machine Word Size
const WORD_SHIFT: uint = #if (ARCH_BITS == 64) 3 #else 2 #end;
const WORD_SIZE: uint = 1 << WORD_SHIFT;

// Page Sizing
const SMALL_PAGE_SHIFT: uint = 13 + WORD_SHIFT;
const SMALL_PAGE_SIZE: uint = 1 << SMALL_PAGE_SHIFT;  // 64 KiB

const MED_PAGE_SHIFT: uint = 3 + SMALL_PAGE_SHIFT;
const MED_PAGE_SIZE: uint = 1 << MED_PAGE_SHIFT;  // 512 KiB

const LARGE_PAGE_SHIFT: uint = 3 + MED_PAGE_SHIFT;
const LARGE_PAGE_SIZE: uint = 1 << LARGE_PAGE_SHIFT;  // 4 MiB

// Segment Sizing
const SEGMENT_SHIFT: uint = LARGE_PAGE_SHIFT;
const SEGMENT_SIZE: uint = 1 << SEGMENT_SHIFT;
const SEGMENT_ALIGN: uint = SEGMENT_SIZE;
const SEGMENT_MASK: uint = SEGMENT_ALIGN - 1;

const SMALL_PAGES_PER_SEGMENT: uint = SEGMENT_SIZE / SMALL_PAGE_SIZE;
const MED_PAGES_PER_SEGMENT: uint = SEGMENT_SIZE / MED_PAGE_SIZE;
const LARGE_PAGES_PER_SEGMENT: uint = SEGMENT_SIZE / LARGE_PAGE_SIZE;

// Allocation Size Limits
const MAX_SMALL_SIZE: uint = SMALL_PAGE_SIZE / 4;    // 16 KiB
const MAX_MED_SIZE: uint = MED_PAGE_SIZE / 4;        // 128 KiB
const MAX_LARGE_SIZE: uint = LARGE_PAGE_SIZE / 2;    // 2 MiB

const MAX_ALLOC_SIZE: uint = 1 << (@sizeof(*u8) - 1);

// Bin Counts
const N_DIRECT_BINS: uint = KB / WORD_SIZE;
const N_REGULAR_BINS: uint = 73;
const BIN_HUGE: uint = N_REGULAR_BINS;
const BIN_FULL: uint = N_REGULAR_BINS + 1;
const N_TOTAL_BINS: uint = N_REGULAR_BINS + 2;

// Page Flags
const PAGE_ZEROED: byte = 1;
const PAGE_FULL: byte = 2;
const PAGE_RETIRED: byte = 4;

// Retirement
const RETIRE_COUNTER_MAX: byte = 16;

/* ------------------------ Allocator Data Structures ----------------------- */

struct MHeap {
    thread_id: int;
    
    direct_bins: [N_DIRECT_BINS]*MPage;
    pages: [N_TOTAL_BINS]MPageQ;

    small_segments: MSegmentQ;
    med_segments: MSegmentQ;

    delayed_free: *MBlock;
}

struct MSegment {
    thread_id: int;
    size: uint;

    prev, next: *MSegment;

    page_kind: MPageKind;
    page_shift: uint;

    n_used_pages: uint;
    n_total_pages: uint;

    // NOTE: Small pages per segment is the maximum number of pages a segment
    // can have. In reality, there may be fewer than this number of pages
    // actually allocated in the header.
    pages: [SMALL_PAGES_PER_SEGMENT]MPage;

    header_size: uint;
}

struct MSegmentQ {
    first, last: *MSegment;
}

enum MPageKind {
    Small,
    Med,
    Large,
    Huge
}

type MThreadFreeList = uint;

enum MThreadFreeMode {
    Normal,
    Delayed,
    Delaying
}

@inline
func MThreadFreeList.get_block() *MBlock {
    unsafe {
        return (*self & ~0b11) as *MBLock;
    }
}

@inline
func MThreadFreeList.get_mode() MThreadFreeMode {
    unsafe {
        return (*self & 0b11) as MThreadFreeMode;
    }
}

@inline
func MThreadFreeList.set_block(block: *MBlock) MThreadFreeList {
    unsafe {
        return (block as MThreadFreeList) | (self.get_mode() as MThreadFreeList);
    }
}

@inline
func MThreadFreeList.set_mode(mode: MThreadFreeMode) MThreadFreeList {
    unsafe {
        return (mode as MThreadFreeList) | (self.get_block() as MThreadFreeList);
    }
}

struct MPage {
    next, prev: *MPage;

    used, cap, reserved: uint;

    block_size: uint;
    alloc_free: *MBlock;
    local_free: *MBlock;
    thread_free: MThreadFreeList;

    start_addr: *u8;

    flags: byte;
    retire_counter: byte;
}

struct MPageQ {
    first, last: *MPage;
    block_size: uint;
}

struct MBlock {
    next: *MBlock;
}

/* --------------------------- Malloc and Realloc --------------------------- */

func MHeap.malloc(size: uint) *u8 {
    // Find a free page to allocate in.
    if size <= KB { // Fast Path
        let direct_bin = get_word_size(size);
        let page = self.direct_bins[direct_bin];

        if page.alloc_free == null { // Fast Path failed
            // Fallback to generic malloc.
            let new_page = self.find_free_page(size);

            // Collection didn't free up current page.
            if page.alloc_free == null {
                // Mark direct bin as full and replace it.
                let bin = get_page_bin(page.block_size);
                self.make_full(&self.pages[bin], page);
                self.replace_direct(direct_bin, new_page);
            }   

            page = new_page;
        }
    } else { // Slow Path
        page = self.find_free_page(size);
    }

    // `find_free_page` fails internally if it runs out of memory, so we don't
    // need to check if page is valid here.  Thus, we can just directly allocate
    // from the `page` we've found.
    return page.pop_free_block();
}

func MHeap.mrealloc(data: *u8, new_size: uint) *u8 {
    // TODO
}

func MHeap.find_free_page(size: uint) {
    // TODO: delayed freeing

    // Get the page's associated page queue.
    let bin = get_page_bin(size);
    let pageq = &self.pages[bin];

    // Search the page queue for pages with free space.
    let page = pageq.first;
    while page != null {
        // Try to collect the page's free lists.
        page.collect_free();

        if page.alloc_free != null { // Found a free page!
            return page;
        }

        // Mark the page as full and move on.
        let next = page.next;
        self.make_full(pageq, page);
        page = next;
    }

    // Otherwise, allocate a fresh page.
    return self.page_alloc(pageq, size);
}

func MHeap.page_alloc(pageq: *MPageQ, size: uint) {
    // Find a segment with a available pages.
    let page_kind = get_page_kind(pageq.block_size);
    // TODO: huge pages using `size`
    let segment = self.find_free_segment(page_kind, SEGMENT_SIZE);

    // Grab a free page from the segment.
    let page: *MPage;
    for let i = 0; i < segment.n_total_pages; i++ {
        page = &segment.pages[i];

        if page.used == 0 {
            break;
        }
    } else { // No free pages: should never happen.
        throw("free segment with no free pages")
    }

    // Commit the page as necessary.
    if page.capacity == 0 {
        sys_commit(page.start_addr, page.reserved, 0);
        page.capacity = page.reserved;
    }

    // Reinitialize the page.
    page.clear();
    page.block_size = pageq.block_size;
    page.init_freelists();

    // Return the freshly allocated page.
    return page;
}

func MHeap.mark_full(pageq: *MPageQ, page: *MPage) {
    // TODO: set thread_free state (handle potential race conditions)

    // Dequeue the page from its current page queue.
    pageq.dequeue(page);

    // Add it to the full queue.
    self.pages[BIN_FULL].enqueue(page);

    // Update its flags.
    page.flags |= PAGE_FULL;
}

func MHeap.replace_direct(direct_bin: uint, new_page: *MPage) {
    // TODO
}

/* ---------------------------------- Free ---------------------------------- */

func MHeap.mfree(data: *u8) {
    // TODO
}

/* ----------------------------- Initialization ----------------------------- */

func MHeap.init(segment: *MSegment) {
    // TODO   
}

/* ----------------------------- Page Management ---------------------------- */

func MPage.pop_free_block() *u8 {
    let block = page.alloc_free;
    page.alloc_free = block.next;
    page.used += page.block_size;

    // Zero out the new memory block.
    if (page.flags & PAGE_ZEROED) > 0 {
        block.next = null;
    } else {
        unsafe {
            memset(block as *u8, 0, page.block_size);
        }
    }

    // Return the allocated memory.
    unsafe {
        return block as *u8;
    }
}

func MPage.zero_init(reserved: uint) {
    self.clear();
    self.cap = 0;
    self.reserved = reserved;
}

func MPage.clear() {
    self.next = null;
    self.prev = null;

    self.used = 0;

    self.flags &= ~(PAGE_FULL | PAGE_RETIRED);
    self.retire_counter = 0;
}

func MPage.init_freelists() {
    let n_blocks = self.cap / self.block_size;

    // Initialize the free list starting from the end and chaining backward.
    let head: *MBlock;
    unsafe {
        head = (self.start_addr as *MBlock) + (n_blocks - 1);
        head.next = null;

        let next = head;
        for let i = 0; i < n_blocks - 1; i++ {
            head--;
            head.next = next;
            next = head;
        }
    }
    self.alloc_free = head;


    // Local and thread free start empty.
    self.local_free = null;
    self.thread_free = null;
}

func MPage.collect_free() {
    // Collect the thread free list: this essentially just prepends the thread
    // free list onto the front of the local free list.
    self.collect_thread_free();

    // We only want to do a collection if we can easily replace the alloc_free
    // list. Otherwise, collection becomes a linear time operation.
    if self.alloc_free == null {
        // Collect the local free list.
        self.alloc_free = self.local_free;
        self.local_free = null;

        // Freed blocks may not be zero now: clear the zeroed flag.
        self.flags &= ~PAGE_ZEROED;
    }
}

func MPage.collect_thread_free() {
    // Swap out the thread free list using an atomic exchange.
    let tfree_head: *MBlock;
    let new_tfree: MThreadFreeList;

    let old_tfree: MThreadFreeList = @atomic_load(&self.thread_free, RELAXED);
    do {
        head = old_tfree.block();
        new_tfree = old_tfree.set_block(null);
    } while @atomic_cas_weak(&self.thread_free, &old_tfree, new_tfree, ACQ_REL, RELAXED);

    // If the thread free list is empty, then we're done.
    if tfree_head == null {
        return;
    }

    // Otherwise, find its tail and count number of elements.
    let tail = head;
    let count = 1;
    while tail.next != null {
        tail = tail.next;
        count++;
    }

    // Chain the local free list onto the tail of thread free list.
    tail.next = self.local_free;
    self.local_free = tail;

    // Update the usage count based on the collection.
    self.used -= count;
}

func MPageQ.enqueue(page: *MPage) {
    page.next = null;

    page.prev = self.last;
    self.last = page;

    if self.first == null {
        self.first = page;
    }
}

func MPageQ.dequeue(page: *MPage) {
    if page.next != null {
        page.next.prev = page.prev;
    }

    if page.prev != null {
        page.prev.next = page.next;
    }

    if self.first == page {
        self.first = page.next;
    }

    if self.last == page {
        self.last = page.prev;
    }
}

/* -------------------------------- Segments -------------------------------- */

func MHeap.find_free_segment(page_kind: MPageKind, seg_size: uint) *MSegment {
    match page_kind {
    case .Small:
        let segment = self.search_segment_queue(&self.small_segments);
        if segment == null {
            segment = msegment_sys_alloc(page_kind, SEGMENT_SIZE);
            self.small_segments.enqueue(segment);
        }

        return segment;
    case .Med:
        let segment = self.search_segment_queue(&self.med_segments);
        if segment == null {
            segment = msegment_sys_alloc(page_kind, SEGMENT_SIZE);
            self.med_segments.enqueue(segment);
        }

        return segment;
    case _:
        return msegment_sys_alloc(page_kind, seg_size);
    }
}

func MHeap.search_segment_queue(sq: *MSegmentQ) *MSegment {
    for let segment = sq.first; segment != null; segment = segment.next {
        if segment.n_used_pages < segment.n_total_pages {
            return segment;
        }
    }

    return null;
}

func MSegmentQ.enqueue(segment: *MSegment) {
    segment.next = null;
    segment.prev = self.last;
    self.last = segment;

    if self.first == null {
        self.first = segment;
    }
}

func msegment_sys_alloc(page_kind: MPageKind, seg_size: uint) *MSegment {
    // Compute page information.
    let page_shift: uint;
    let n_pages: uint;

    match page_kind {
    case .Small:
        page_shift = SMALL_PAGE_SHIFT;
        n_pages = SMALL_PAGES_PER_SEGMENT;
    case .Med:
        page_shift = MED_PAGE_SHIFT;
        n_pages = MED_PAGES_PER_SEGMENT;
    case .Large:
        page_shift = LARGE_PAGE_SHIFT;
        n_pages = LARGE_PAGES_PER_SEGMENT;
    case .Huge:
        throw("huge pages not implemented");
    }

    let page_size = 1 << page_shift;

    // Allocate the segment.
    let segment: *MSegment;
    let full_commit = false;
    unsafe {
        // Try to reserve the whole region.
        segment = sys_reserve(seg_size, SEGMENT_ALIGN) as *MSegment;
        if segment == null {
            // Reserve may not be implemented on every system, so we fallback to
            // sys_commit if it fails: on such systems, we commit the full
            // segment at once.
            segment = sys_commit(null, seg_size, SEGMENT_ALIGN) as *MSegment;
            full_commit = true;
        } else {
            // Otherwise, we only commit the first page of the segment and leave
            // the others as reserved.  That way, we can lazily allocate more
            // pages as we need them.
            segment = sys_commit(segment as *u8, page_size, 0) as *MSegment;
        }
    }

    if segment == null {
        panic("failed to allocate segment: out of memory");
    }

    // Initialize the segment info.
    segment.thread_id = get_thread_id();
    segment.size = seg_size;
    segment.prev = null;
    segment.next = null;

    segment.page_kind = page_kind;
    segment.page_shift = page_shift;

    segment.n_used_pages = 0;
    segment.n_total_pages = n_pages;

    // Determine the size of the segment header.
    let header_size = @sizeof(MSegment) - @sizeof([SMALL_PAGES_PER_SEGMENT]MPage);
    header_size += n_pages * @sizeof(MPage);
    segment.header_size = header_size;

    // Part of the first page is used for the header.
    let first_page_size = page_size - header_size;

    // Initialize the first page.
    let page = &segment.pages[0];
    page.zero_init(first_page_size);
    page.cap = first_page_size;
    unsafe {
        page.start_addr = (segment as *u8) + header_size;
    }

    // Initialize the remaining pages.
    for let i = 1; i < n_pages; i++ {
        page = &segment.pages[i];
        page.zero_init(page_size);
        
        unsafe {
            page.start_addr = (segment as *u8) + i * page_size;
        }
    }

    return segment;
}

/* -------------------------------------------------------------------------- */

func get_page_kind(size: uint) MPageKind {
    if size <= MAX_SMALL_SIZE {
        return .Small;
    } elif size <= MAX_MED_SIZE {
        return .Med;
    } elif size <= MAX_LARGE_SIZE {
        return .Large;
    } elif size <= MAX_ALLOC_SIZE {
        return .Huge;
    } else {
        throw("allocation too large");
    }
}

func get_page_bin(size: uint) uint {
    let wsize = get_word_size(size);

    if wsize <= 1 {
        return 0;
    }
}

@inline
func get_word_size(size: uint) uint {
    return (size + 1) / WORD_SIZE;
}