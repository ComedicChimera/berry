module runtime;

/* ------------------------------ External API ------------------------------ */

func _malloc(size: uint) *u8 {
    let rs = rtGetState();
    let flags = rs.swapFlags(RS_FLAG_THROW);

    let data = rs.heap.malloc(size);

    rs.flags = flags;
    return data;
}

func _mrealloc(data: *u8, new_size: uint) *u8 {
    let rs = rtGetState();
    let flags = rs.swapFlags(RS_FLAG_THROW);

    data = rs.heap.realloc(data, new_size);

    rs.flags = flags;
    return data;
}

func _mfree(data: *u8) {
    let rs = rtGetState();
    let flags = rs.swapFlags(RS_FLAG_THROW);

    rs.heap.free(data);

    rs.flags = flags;
}

/* -------------------------------- Constants ------------------------------- */

// Machine Word Sizes
const M_WORD_SHIFT: uint = #if (ARCH_BITS == 64) 3 #else 2 #endif;
const M_WORD_SIZE: uint = 1 << M_WORD_SHIFT;

const M_MIN_ALIGN: uint = 16;                              // Minimum Allocation Alignment (16-bit on most platforms)
const M_MIN_WORD_ALIGN: uint = M_MIN_ALIGN / M_WORD_SIZE;  // 2 (on 64-bit)

// Page Sizes
const M_SMALL_PAGE_SHIFT: uint = 3 + M_WORD_SHIFT;
const M_MED_PAGE_SHIFT: uint = 3 + M_SMALL_PAGE_SHIFT;
const M_LARGE_PAGE_SHIFT: uint = 3 + M_MED_PAGE_SHIFT;

const M_SMALL_PAGE_SIZE: uint = 1 << M_SMALL_PAGE_SHIFT;  // 64 KiB (on 64-bit)
const M_MED_PAGE_SIZE: uint = 1 << M_MED_PAGE_SHIFT;      // 512 KiB 
const M_LARGE_PAGE_SIZE: uint = 1 << M_LARGE_PAGE_SHIFT;  // 4 MiB

// Segment Sizes
const M_SEGMENT_SHIFT: uint = M_LARGE_PAGE_SHIFT;
const M_SEGMENT_SIZE: uint = M_LARGE_PAGE_SIZE;
const M_SEGMENT_ALIGN: uint = M_SEGMENT_SIZE;
const M_SEGMENT_MASK: uint = M_SEGMENT_ALIGN - 1;

const M_SMALL_PAGES_PER_SEGMENT: uint = M_SEGMENT_SIZE / M_SMALL_PAGE_SIZE;  // 64
const M_MED_PAGES_PER_SEGMENT: uint = M_SEGMENT_SIZE / M_MED_PAGE_SIZE;      // 8
const M_LARGE_PAGES_PER_SEGMENT: uint = M_SEGMENT_SIZE / M_LARGE_PAGE_SIZE;  // 1

// Allocation Size Limits
const M_MAX_SMALL_OBJ_SIZE: uint = M_SMALL_PAGE_SIZE / 4;   // 16 KiB (on 64-bit)
const M_MAX_MED_OBJ_SIZE: uint = M_MED_PAGE_SIZE / 4;       // 128 KiB
const M_MAX_LARGE_OBJ_SIZE: uint = M_LARGE_PAGE_SIZE / 2;   // 2 MiB

const M_MAX_ALLOC_SIZE: uint = #if (ARCH_BITS == 64) (1 << 48) - 1 #else (1 << 31) - 1 #end;

// Bin Counts
const M_N_REG_BINS: uint = 72;                  // 72 regular size classes
const M_BIN_HUGE: uint = M_N_REG_BINS;          // Bin 72 = Huge Bin
const M_BIN_FULL: uint = M_BIN_HUGE + 1;        // Bin 73 = Full List
const M_N_TOTAL_BINS: uint = M_N_REG_BINS + 2;  // Total Bins = 72 + 1 (huge bin) + 1 (full list)

const M_N_DIRECT_BINS: uint = 128;                              // Number of Bins in `direct_pages`
const M_MAX_DIRECT_SIZE: uint = M_WORD_SIZE * M_N_DIRECT_BINS;  // = 1 KiB on 64-bit systems

// Page Flags
const M_PAGE_ZEROED: uint = 1;
const M_PAGE_RESET: uint = 2;
const M_PAGE_FULL: uint = 4;
const M_PAGE_RETIRED: uint = 8;

// Retirement and Revival
const M_REVIVAL_CHANCES_SMALL: uint = 16;
const M_REVIVAL_CHANCES: uint = M_REVIVAL_CHANCES_SMALL / 4;

/* ------------------------------- Allocation ------------------------------- */

struct MHeap {
    thr_id: uint;

    direct_pages: [M_N_DIRECT_BINS]*MPage;
    pages: [M_N_TOTAL_BINS]MPageQ;

    xdelayed_free: *MBlock;

    retire_ndx_min, retire_ndx_max: uint;
}

func MHeap.malloc(size: uint) *u8 {
    // Find a page with free space.
    let page: *MPage = null;
    if size <= M_MAX_DIRECT_SIZE { // Fast Path
        let wsize = mGetWordSize(size);
        page = self.direct_pages[wsize];

        if page.alloc_free == null { // Fast Path failed
            // Fallback to generic malloc
            page = self.findFreePage(size);
        }
    } elif size > M_MAX_ALLOC_SIZE { // Huge Allocation
        page = self.allocHugePage(size);
    } else { // Regular (slow) Path
        page = self.findFreePage(size);
    }

    // NOTE: page is never null here.

    // Allocate in the free page.
    return page.popFreeBlock();
}

func MHeap.realloc(data: *u8, new_size: uint) *u8 {
    // TODO
    return null;
}

func MHeap.findFreePage(size: uint) *MPage {
    // Collect all delayed frees to free up space.
    self.collectDelayed(false /* no force*/);

    // Get the page queue associated with size.
    let bin = mGetPageBin(size);
    let pq = &self.pages[bin];

    // Search the page queue for a free page.
    let page = pq.first;
    while page != null {
        // Collect page's free lists.
        page.collectFree();

        if page.alloc_free != null { // Page has free space?
            // Done: reuse page with free space.
            return page;
        }

        // Pass is full: move to full list.
        let next = page.next;
        self.moveToFull(pq, page);
        page = next;
    }

    // Collect retired pages to free up space.
    self.collectRetired(false /* no force */);

    page = self.allocPage(pq);
    if page == null { // Allocation failed
        // Collect all delayed frees (including page's with high contention).
        self.collectDelayed(true /* force */);
        
        // Force collect all retired pages (including pages with remaining
        // revival chances: free them early).
        self.collectRetired(true /* force */);

        // Try to allocate again.
        page = self.allocPage(pq);  
        if page == null { // Out of memory.
            panic("out of memory");
        }
    }

    return page;
}

func MHeap.collectDelayed(force: bool) {
    // Take ownership of the delayed free list.
    let block = @atomic_load(&self.xdelayed_free, AMO_RELAXED);
    while block != null && !@atomic_cas_weak(&self.xdelayed_free, block, null, AMO_ACQ_REL, AMO_RELAXED) {}

    while block != null {
        let next = block.next;

        if !self.tryFreeDelayed(block, force) { // Too much contention to free block.
            // Push the block back onto the delayed free list.
            let dfree = @atomic_load(&self.xdelayed_free, AMO_RELAXED);
            do {
                block.next = dfree;
            } while !@atomic_cas_weak(&self.xdelayed_free, dfree, block, AMO_RELEASE, AMO_RELAXED);
        }

        block = next;
    }
}

func MHeap.collectRetired(force: bool) {
    // Keep track of min and max indices, so we can keep them updated.
    let new_min = retire_ndx_max;
    let new_max = retire_ndx_min;

    for let i = self.retire_ndx_min; i <= retire_ndx_max; i++ {
        // Only the first page in a queue can be retired.
        let page = pages[i].first;

        if page != null && (page.flags & M_PAGE_RETIRED) > 0 {
            // If the page has been allocated in, then we should revive it.
            if page.n_used_blocks > 0 {
                page.flags &= ~M_PAGE_RETIRED;
                page.revival_chances = 0;
                continue;
            }

            page.revival_chances--;
            if force || page.revival_chances == 0 { // Page is out of chances.
                self.freePage(&pages[i], page);
            } else { // Still retired.
                if (i < new_min) {
                    new_min = i;
                } 

                if (i > new_max) {
                    new_max = i;
                }
            }
        }
    }

    // Update retirement indices.
    retire_ndx_min = new_min;
    retire_ndx_max = new_max;
}

func MHeap.allocPage(pq: *MPageQ) *MPage {
    // Try to find a segment with free pages for allocation.
    let segment = self.findOrAllocFreeSegment(pq.block_size);
    if segment == null {
        return null;
    }

    // Find the first free page in the segment.
    let page: *MPage = null;
    for let i = 0; i < segment.n_total_pages; i++ {
        page = &segment.pages[i];

        if page.n_used_blocks == 0 {
            break;
        }
    } else { // Should not happen.
        throw("allocator: free segment with no free pages");
    }

    segment.n_used_pages++;
    if segment.n_used_pages == segment.n_total_pages { // Segment is full.
        self.dequeueFreeSegment(segment);
    }

    // Prepare the page for allocation.
    page.block_size = block_size;
    page.makeReady();

    self.enqueuePage(pq, page);
    return page;
}

func MHeap.allocHugePage(size: uint) *MPage {
    // Compute the actual size needed for allocation.
    let segment_size = @sizeof(MSegment) - (M_SMALL_PAGES_PER_SEGMENT - 1) * @sizeof(MPage);
    let full_size = sysAlignHugePageSize(size + segment_size);
    if full_size > M_MAX_ALLOC_SIZE {
        panic("requested allocation size is too large");
    }

    // Allocate the backing segment.
    let segment = mSegmentAlloc(self.thr_id, full_size, .Huge);

    // Allocate the first (and only) page of the segment.
    let page = &segment.pages[0];
    segment.n_used_pages++;

    page.block_size = mAlignUp(size, M_MIN_ALIGN);
    page.makeReady();

    self.enqueuePage(&self.pages[M_BIN_HUGE], page);
    return page;
}

@inline
func mAlignUp(size, align: uint) uint {
    // NOTE: Align must be a power of 2.
    return (size + align - 1) & ~(align - 1);
}

/* --------------------------------- Freeing -------------------------------- */

func MHeap.free(data: *u8) {
    let block = unsafe(data as *MBlock);

    // Get the parent segment and page.
    let segment = mGetParentSegment(block);
    let page = segment.getParentPage(block);

    // Check if this a local or cross-thread free.
    if @atomic_load(segment.xthr_id, AMO_RELAXED) == self.thr_id || self.tryReclaim(segment) {
        self.freeLocal(page, block);
    } else {
        self.freeCross(page, block);
    }
}

func MHeap.freeLocal(page: *MPage, block: *MBlock) {
    // Add the block to the page's free list.
    block.next = page.local_free;
    page.local_free = block;
    page.n_used_blocks--;

    // Page is no longer zeroed.
    page.flags &= ~M_PAGE_ZEROED;

    if page.n_used_blocks == 0 { // Page is empty.
        self.freeOrRetirePage(page);
    } elif (page.flags & M_PAGE_FULL) > 0 { // Page is now unfull.
        self.removeFromFull(page);
    }
}

func MHeap.freeCross(page: *MPage, block: *MBlock) {
    let no_delay: bool;

    let tfree = @atomic_load(&page.xthr_free, AMO_RELAXED);
    let new_tfree: MThrFreeList;
    do {
        no_delay = tfree.getDelay() != .Delayed;
        if no_delay { // Fast Path
            // Push onto thread free list.
            block.next = tfree.getBlock();
            new_tfree = tfree.setBlock(block);
        } else { // Slow Path
            // Take ownership of delayed list.
            new_tfree = tfree.setDelay(.Delaying);
        }
    } while !@atomic_cas_weak(&page.xthr_free, &tfree, new_tfree, AMO_RELEASE, AMO_RELAXED);

    if no_delay { // Fast Path
        // Already pushed onto thread free list: done!
        return;
    }

    // Push onto delayed free list.
    let dfree = @atomic_load(&self.xdelayed_free, AMO_RELAXED);
    do {
        block.next = dfree;
    } while !@atomic_cas_weak(&self.xdelayed_free, &dfree, block, AMO_RELEASE, AMO_RELAXED);

    // Clear the delayed status on the thread free list.
    tfree = @atomic_load(&page.xthr_free, AMO_RELAXED);
    do {
        new_tfree = tfree.setDelay(.NoDelay);
    } while !@atomic_cas_weak(&page.xthr_free, &tfree, new_tfree, AMO_RELEASE, AMO_RELAXED);
}

func MHeap.tryFreeDelayed(block: *MBlock, force: bool) bool {
    // Get the segment and page.
    let segment = mGetParentSegment(block);
    let page = segment.getParentPage(block);

    // This step is a bit subtle.  The general idea is that since we are
    // potentially removing a page's last "representative" block from the
    // delayed list, we want to reinstate the "delayed" flag, return to default
    // behavior.  If we didn't do this, we could end up with a case where
    // cross-thread frees linger in the page's thread free list until it happens
    // to be scanned when we are searching for free pages.  In that case, a page
    // could sit allocated for a substantially long time without being purged
    // even if its space isn't really being used.  Resetting the delayed flag
    // here avoids that by forcing subsequent frees to fallback to the delayed
    // mode thereby ensuring the page always has a block in the delayed list if
    // it is experiencing cross-thread freeing.
    if !page.waitAndSetDelay(.Delayed, force) {
        // If there is too much contention on the list, then we can just fail to
        // free the block and leave it in the delayed list for later.  This way,
        // we don't miss any cross-thread frees but also aren't stuck waiting
        // forever to get to free our block.
        return false;
    }

    // Collect all the other non-local frees to ensure an update-to-date usage
    // count, so we can check whether the page can be freed.
    page.collectThreadFree();

    // Free the delayed block as a local block.
    self.freeLocal(block);
    return true;
}

func MHeap.freeOrRetirePage(page: *MPage) {
    // Get the page's associate page queue.
    let pq = self.getPageQueue(page);

    if pq.block_size == 0 { // Special page queue
        self.freePage(pq, page);
    } elif pq.first == page && pq.last == page { // Page queue contains only 1 page
        self.retirePage(pq, page);
    } else { // Page queue contains more than 1 page
        self.freePage(pq, page);
    }
}

func MHeap.freePage(pq: *MPageQ, page: *MPage) {
    // Remove the page from its parent page queue.
    self.dequeuePage(pq, page);

    // Make sure the page is properly cleared.
    page.flags = 0;
    page.n_used_blocks = 0;
    page.revival_chances = 0;

    // Update the parent segment's usage count.
    let segment = mGetParentSegment(unsafe(page as *MBlock));

    if segment.n_used_pages == segment.n_total_pages { // Segment was full.
        self.enqueueFreeSegment(segment);
    }
    segment.n_used_pages--;

    if segment.n_used_pages == 0 { // Segment is empty.
        self.freeSegment(segment);
    } else { // Segment is still in use.
        // Reset the page.
        sysReset(page.start, page.capacity);
        page.flags = M_PAGE_RESET;
    }
}

func MHeap.retirePage(pq: *MPageQ, page: *MPage) {
    // Mark the page as retired.
    page.flags |= M_PAGE_RETIRED;

    // Set its number of revival chances.
    if page.block_size > M_MAX_SMALL_OBJ_SIZE {
        page.revival_chances = M_REVIVAL_CHANCES;
    } else {
        page.revival_chances = M_REVIVAL_CHANCES_SMALL;
    }

    // Update the retirement indices.
    let ndx = self.getQueueIndex(pq);
    if ndx < self.retire_ndx_min {
        self.retire_ndx_min = ndx;
    }

    if ndx > self.retire_ndx_max {
        self.retire_ndx_max = ndx;
    }
}

/* ----------------------- Initialization and Cleanup ----------------------- */

let _mpage_empty: MPage = null;

func MHeap.init() {
    // TODO
}

func MHeap.cleanup() {
    // TODO
}

/* ------------------------------ Page Queuing ------------------------------ */

struct MPageQ {
    first, last: *MPage;
    block_size: uint;
}

func MHeap.moveToFull(src_pq: *MPageQ, page: *MPage) {
    // Remove the page from its old page queue.
    dequeuePage(src_pq, page);

    // Mark it as full.
    page.flags |= M_PAGE_FULL;

    // Add it to the full list.
    enqueuePage(&self.pages[M_BIN_FULL], page);
}

func MHeap.removeFromFull(page: *MPage) {
    // Remove the page from the full list.
    dequeuePage(&self.pages[M_BIN_FULL], page);

    // Remove its full flag.
    page.flags &= ~M_PAGE_FULL;

    // Add it to its destination queue.
    let dest_pq = self.getPageQueue(page);
    enqueuePage(dest_pq, page);
}

func MHeap.enqueuePage(pq: *MPageQ, page: *MPage) {
    // Link the page into the queue.
    page.next = null;
    page.prev = pq.last;
    pq.last = page;

    if pq.first == null { // Queue was empty
        // page is now also front of queue.
        pq.first = page;
        self.updateQueueFirst(pq);
    }
}

func MHeap.dequeuePage(pq: *MPageQ, page: *MPage) {
    // Remove page from the chain by updating prev and next.
    if page.next != null {
        page.next.prev = page.prev;
    }

    if page.prev != null {
        page.prev.next = page.next;
    }

    if pq.first == page { // page was the first element
        // First element is now page's next element.
        pq.first = page.next;
        self.updateQueueFirst(pq);
    }

    if pq.last == page { // page was the last element.
        // Last element is now page's previous element.
        pq.last = page.prev;
    }

    // Reset page's prev and next pointers.
    page.next = null;
    page.prev = null;
}

func MHeap.updateQueueFirst(pq: *MPageQ) {
    if pq.block_size == 0 || pq.block_size > M_MAX_DIRECT_SIZE { // Special or bigger than direct
        // No update needed.
        return;
    }

    // Get the new first page.
    let page = pq.first;
    if page == null {
        // If the first is empty, then we want the direct bins to point to the
        // empty page instead of just being a null pointer.  This avoids an
        // extra comparison on the fast path during allocation.
        page = &_mpage_empty;
    }

    // Check whether the direct bin has already been updated.
    let end = mGetWordSize(pq.block_size);
    if page == direct_pages[end] {
        return;
    }

    // Determine the start index of the range of bins that should be updated.
    let start = 0;
    if end > 1 { // Not first bin
        // Work out the size of the previous valid bin.  On a some platforms,
        // certain bins are unused due to alignment considerations.  Hence, we
        // want to skip until we find the first page queue whose block size
        // corresponds to an actually usable bin.
        let bin = mGetPageBin(bin);

        let prev = pq;
        unsafe {
            prev--;

            while prev > &self.pages[0] && bin == mGetPageBin(prev.block_size) {
                prev--;
            }
        }

        // The start index for pq's direct bins will be the word size of the
        // previous usable bin + 1 because the previous bin will be used for
        // direct bins with size <= prev.block_size.
        start = 1 + mGetWordSize(prev.block_size);

        // If the start index is too large (due to rounding), just set it equal
        // to the end index: we will be replacing only one direct bin.
        if start > end {
            start = end;
        }
    }

    // Replace all associated direct bins with the new page.
    for let i = start; i <= end; i++ {
        self.direct_pages[i] = page;
    }
}

func MHeap.getPageQueue(page: *MPage) *MPageQ {
    if (page.flags & M_PAGE_FULL) > 0 { // Full list
        return &self.pages[M_BIN_FULL];
    }

    let bin = mGetPageBin(page.block_size);
    return &self.pages[bin];
}

@inline
func MHeap.getQueueIndex(pq: *MPageQ) uint {
    unsafe {
        return pq - &self.pages[0] as uint;
    }
}

func mGetPageBin(size: uint) uint {
    // TODO
}

@inline
func mGetWordSize(size: uint) uint {
    return (size + M_WORD_SIZE - 1) / M_WORD_SIZE;
}

/* ---------------------------------- Pages --------------------------------- */

type MThrFreeList = uint;

enum MThrDelay {
    Delayed = 0;
    Delaying = 1;
    NoDelay = 2;
    NeverDelay = 3;
}

func MThrFreeList.getBlock() *MBlock {
    unsafe {
        return (*self & ~3) as *MBlock;
    }
}

func MThrFreeList.setBlock(block: *MBlock) MThrFreeList {
    unsafe {
        return (*self & 3) | (block as uint);
    }
}

func MThrFreeList.getDelay() MThrDelay {
    unsafe {
        return (*self & 3) as MThrDelay;
    }
}

func MThrFreeList.setDelay(delay: MThrDelay) MThrFreeList {
    return (*self & ~3) | (delay as uint);
}

struct MPage {
    flags: u8;
    revival_chances: u8;

    block_size: uint;

    prev, next: *MPage;

    n_used_blocks: uint;
    capacity, reserved: uint;

    alloc_free, local_free: *MBlock;
    xthr_free: MThrFreeList;

    start: *u8;
}

struct MBlock {
    next: *MBlock;
}

func MPage.popFreeBlock() *u8 {
    // Pop a block from the front of the allocation list.
    let block = self.alloc_free;
    self.alloc_free = block.next;
    self.n_used_blocks++;

    let data = unsafe(block as *u8);

    // Zero the block.
    if (self.flags & M_PAGE_ZEROED) > 0 {
        block.next = null;
    } else {
        memset(data, 0, self.block_size);
    }

    return data;
}

func MPage.collectFree() {
    // Collect thread free list.
    self.collectThreadFree();

    // Collect local free list.
    if self.alloc_free == null {
        self.alloc_free = self.local_free;
        self.local_free = null;
    }
}

func MPage.collectThreadFree() {
    // Atomically acquire the thread list.
    let tfree = @atomic_load(&self.xthr_free, AMO_RELAXED);
    let new_tfree: MThrFreeList = 0;
    let head: *MBlock = null;
    do {
        head = tfree.getBlock();
        new_tfree = tfree.setBlock(null);
    } while @atomic_cas_weak(&self.xthr_free, &tfree, new_tfree, AMO_ACQ_REL, AMO_RELAXED);

    // Thread free list is empty: nothing to do.
    if head == null {
        return;
    }

    // Find the tail of the thread free list and update the usage count.
    let tail = head;
    self.n_used_blocks--;
    while tail.next != null {
        tail = tail.next;
        self.n_used_blocks--;
    }

    // Concatenate the thread free list onto the front of the local list.
    tail.next = self.local_free;
    self.local_free = head;
}

func MPage.waitAndSetDelay(delay: MThrDelay, force: bool) bool {
    let attempts = 0;

    let tfree: MThrFreeList;
    let new_tfree: MThrFreeList;
    let old_delay: MThrDelay;

    do {
        // Load with acquire since we may repeat/exit loop without doing a CAS.
        tfree = @atomic_load(&self.xthr_free, AMO_ACQUIRE);
        old_delay = tfree.getDelay();

        if old_delay == .Delaying { // Other thread is working on list: can't change delay.
            // If not forced, give up after 4 tries.
            if !force && attempts > 4 { // Too much contention.
                return false;
            }
            attempts++;

            // Yield and try again.
            thrYield();
            continue;
        } elif old_delay == .NeverDelay {
            // Never delay can't be overwritten.
            return false;
        } elif old_delay == delay {
            // No more work to be done: delay is already correct.
            return true;
        }
        
        // Update delay.
        new_tfree = tfree.setDelay(delay);
    } while !@atomic_cas_weak(&self.xthr_free, &tfree, new_tfree, AMO_RELEASE, AMO_RELAXED);

    return true;
}

func MPage.enableDelayedFree() {
    let tfree = @atomic_load(&self.xthr_free, AMO_RELAXED);
    let new_tfree: MThrFreeList;

    do {
        new_tfree = tfree.setDelay(.Delayed);
    } while @atomic_cas_weak(&self.xthr_free, &tfree, new_tfree, AMO_RELEASE, AMO_RELAXED);
}

/* -------------------------------- Segments -------------------------------- */

enum MPageKind {
    Small,
    Med,
    Large,
    Huge
}

struct MSegment {
    xthr_id: uint;
    full_size: uint;

    prev, next: *MSegment;

    page_shift: uint;
    page_kind: MPageKind;

    n_used_pages, n_total_pages: uint;
    n_abandoned_pages: uint;

    // Maximum # of pages = 64; could be less.  We only actually allocate the
    // space we need, so bounds-checking on this array is really more of a
    // sanity check than anything else.
    pages: [M_SMALL_PAGES_PER_SEGMENT]MPage;

    header_size: uint;
}

func mGetParentSegment(block: *MBlock) *MSegment {
    unsafe {
        return (block as uint) & ~SEGMENT_MASK as *MSegment;
    }
}

func MSegment.getParentPage(block: *MBlock) {
    // TODO
}
