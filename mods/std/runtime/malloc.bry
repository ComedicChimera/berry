module runtime;

/* ------------------------ Berry's Memory Allocator ------------------------ */

// Berry's memory allocator (berry-malloc) is a simplified implementation of
// Microsoft's mi-malloc (https://github.com/microsoft/mimalloc).  It is
// specifically designed to support only the use cases which occur in Berry.  A
// design overview of berry-malloc can be found in docs/malloc.md.   

/* ------------------------------ External API ------------------------------ */

pub func _malloc(size: uint) *u8 {
    let rs = rtGetState();
    let flags = rs.swapFlags(RS_FLAG_THROW);

    let data = rs.heap.malloc(size);

    rs.flags = flags;
    return data;
}

pub func _mrealloc(data: *u8, new_size: uint) *u8 {
    let rs = rtGetState();
    let flags = rs.swapFlags(RS_FLAG_THROW);

    data = rs.heap.realloc(data, new_size);

    rs.flags = flags;
    return data;
}

pub func _mfree(data: *u8) {
    let rs = rtGetState();
    let flags = rs.swapFlags(RS_FLAG_THROW);

    rs.heap.free(data);

    rs.flags = flags;
}

/* -------------------------------- Constants ------------------------------- */

// Machine Word Sizes
const M_WORD_SHIFT: uint = #if (ARCH_SIZE == 64) 3 #else 2 #end;
const M_WORD_SIZE: uint = 1 << M_WORD_SHIFT;

const M_MIN_ALIGN: uint = 16;                              // Minimum Allocation Alignment (16-bit on most platforms)
const M_MIN_WORD_ALIGN: uint = M_MIN_ALIGN / M_WORD_SIZE;  // 2 (on 64-bit)

// Page Sizes
const M_SMALL_PAGE_SHIFT: uint = 13 + M_WORD_SHIFT;
const M_MED_PAGE_SHIFT: uint = 3 + M_SMALL_PAGE_SHIFT;
const M_LARGE_PAGE_SHIFT: uint = 3 + M_MED_PAGE_SHIFT;

const M_SMALL_PAGE_SIZE: uint = 1 << M_SMALL_PAGE_SHIFT;  // 64 KiB (on 64-bit)
const M_MED_PAGE_SIZE: uint = 1 << M_MED_PAGE_SHIFT;      // 512 KiB 
const M_LARGE_PAGE_SIZE: uint = 1 << M_LARGE_PAGE_SHIFT;  // 4 MiB

// Segment Sizes
const M_SEGMENT_SHIFT: uint = M_LARGE_PAGE_SHIFT;
const M_SEGMENT_SIZE: uint = M_LARGE_PAGE_SIZE;
const M_SEGMENT_ALIGN: uint = M_SEGMENT_SIZE;
const M_SEGMENT_MASK: uint = M_SEGMENT_ALIGN - 1;

const M_SMALL_PAGES_PER_SEGMENT: uint = M_SEGMENT_SIZE / M_SMALL_PAGE_SIZE;  // 64
const M_MED_PAGES_PER_SEGMENT: uint = M_SEGMENT_SIZE / M_MED_PAGE_SIZE;      // 8
const M_LARGE_PAGES_PER_SEGMENT: uint = M_SEGMENT_SIZE / M_LARGE_PAGE_SIZE;  // 1

// Allocation Size Limits
const M_MAX_SMALL_OBJ_SIZE: uint = M_SMALL_PAGE_SIZE / 4;   // 16 KiB (on 64-bit)
const M_MAX_MED_OBJ_SIZE: uint = M_MED_PAGE_SIZE / 4;       // 128 KiB
const M_MAX_LARGE_OBJ_SIZE: uint = M_LARGE_PAGE_SIZE / 2;   // 2 MiB

const M_MAX_ALLOC_SIZE: uint = #if (ARCH_SIZE == 64) (1 << 48) - 1 #else (1 << 31) - 1 #end;

// Bin Counts
const M_N_REG_BINS: uint = 72;                  // 72 regular size classes
const M_BIN_HUGE: uint = M_N_REG_BINS;          // Bin 72 = Huge Bin
const M_BIN_FULL: uint = M_BIN_HUGE + 1;        // Bin 73 = Full List
const M_N_TOTAL_BINS: uint = M_N_REG_BINS + 2;  // Total Bins = 72 + 1 (huge bin) + 1 (full list)

const M_N_DIRECT_BINS: uint = 128;                              // Number of Bins in `direct_pages`
const M_MAX_DIRECT_SIZE: uint = M_WORD_SIZE * M_N_DIRECT_BINS;  // = 1 KiB on 64-bit systems

// Page Flags
const M_PAGE_ZEROED: u32 = 1;
const M_PAGE_RESET: u32 = 2;
const M_PAGE_FULL: u32 = 4;
const M_PAGE_RETIRED: u32 = 8;

// Retirement and Revival
const M_REVIVAL_CHANCES_SMALL: u32 = 16;
const M_REVIVAL_CHANCES: u32 = M_REVIVAL_CHANCES_SMALL / 4;

/* ------------------------------- Allocation ------------------------------- */

// MHeap represents the thread-local state of the allocator.  All allocations
// and deallocations in a specific thread are done through MHeap.
struct MHeap {
    // The id of the parent thread.
    thr_id: uint;

    // Free segment queues.
    small_free_segments: MSegmentQ;
    med_free_segments: MSegmentQ;

    // Page queues organized by size-class.
    pages: [M_N_TOTAL_BINS]MPageQ;

    // Direct pages array organized in increments of the machine word-size for
    // fast small allocation.  The pages used in this array correspond to pages
    // allocated in the the page queues.
    direct_pages: [M_N_DIRECT_BINS]*MPage;

    // The heap's delayed free list.
    xdelayed_free: *MBlock;

    // Min and max indices of queue's containing retired pages.
    retire_ndx_min, retire_ndx_max: uint;
}

func MHeap.malloc(size: uint) *u8 {
    // Find a page with free space.
    let page: *MPage = null;
    if size <= M_MAX_DIRECT_SIZE { // Fast Path
        let wsize = mGetWordSize(size);
        page = self.direct_pages[wsize];

        if page.alloc_free == null { // Fast Path failed
            // Fallback to generic malloc
            page = self.findFreePage(size);
        }
    } elif size > M_MAX_ALLOC_SIZE { // Huge Allocation
        page = self.allocHugePage(size);
    } else { // Regular (slow) Path
        page = self.findFreePage(size);
    }

    // NOTE: page is never null here.

    // Allocate in the free page.
    return page.popFreeBlock();
}

func MHeap.realloc(data: *u8, new_size: uint) *u8 {
    // TODO
    return null;
}

func MHeap.findFreePage(size: uint) *MPage {
    // Collect all delayed frees to free up space.
    self.collectDelayed(false /* no force*/);

    // Get the page queue associated with size.
    let bin = mGetPageBin(size);
    let pq = &self.pages[bin];

    // Search the page queue for a free page.
    let page = pq.first;
    while page != null {
        // Collect page's free lists.
        page.collectFree();

        if page.alloc_free != null { // Page has free space?
            // Done: reuse page with free space.
            return page;
        }

        // Pass is full: move to full list.
        let next = page.next;
        self.moveToFull(pq, page);
        page = next;
    }

    // Collect retired pages to free up space.
    self.collectRetired(false /* no force */);

    page = self.allocPage(pq);
    if page == null { // Allocation failed
        // Collect all delayed frees (including page's with high contention).
        self.collectDelayed(true /* force */);
        
        // Force collect all retired pages (including pages with remaining
        // revival chances: free them early).
        self.collectRetired(true /* force */);

        // Try to allocate again.
        page = self.allocPage(pq);  
        if page == null { // Out of memory.
            panic("out of memory");
        }
    }

    return page;
}

func MHeap.collectDelayed(force: bool) {
    // Take ownership of the delayed free list.
    let block = @atomic_load(&self.xdelayed_free, AMO_RELAXED);
    while block != null && !@atomic_cas_weak(&self.xdelayed_free, &block, null, AMO_ACQ_REL, AMO_RELAXED) {}

    while block != null {
        let next = block.next;

        if !self.tryFreeDelayed(block, force) { // Too much contention to free block.
            // Push the block back onto the delayed free list.
            let dfree = @atomic_load(&self.xdelayed_free, AMO_RELAXED);
            do {
                block.next = dfree;
            } while !@atomic_cas_weak(&self.xdelayed_free, &dfree, block, AMO_RELEASE, AMO_RELAXED);
        }

        block = next;
    }
}

func MHeap.collectRetired(force: bool) {
    // Keep track of min and max indices, so we can keep them updated.
    let new_min = self.retire_ndx_max;
    let new_max = self.retire_ndx_min;

    for let i = self.retire_ndx_min; i <= self.retire_ndx_max; i++ {
        // Only the first page in a queue can be retired.
        let page = self.pages[i].first;

        if page != null && (page.flags & M_PAGE_RETIRED) > 0 {
            // If the page has been allocated in, then we should revive it.
            if page.n_used_blocks > 0 {
                page.flags &= ~M_PAGE_RETIRED;
                page.revival_chances = 0;
                continue;
            }

            page.revival_chances--;
            if force || page.revival_chances == 0 { // Page is out of chances.
                self.freePage(&self.pages[i], page);
            } else { // Still retired.
                if (i < new_min) {
                    new_min = i;
                } 

                if (i > new_max) {
                    new_max = i;
                }
            }
        }
    }

    // Update retirement indices.
    self.retire_ndx_min = new_min;
    self.retire_ndx_max = new_max;
}

func MHeap.allocPage(pq: *MPageQ) *MPage {
    // Try to find a segment with free pages for allocation.
    let segment = self.findOrAllocFreeSegment(pq.block_size);
    if segment == null {
        return null;
    }

    // Find the first free page in the segment.
    let page: *MPage = null;
    for let i: uint = 0; i < segment.n_total_pages; i++ {
        page = &segment.pages[i];

        if page.n_used_blocks == 0 {
            break;
        }
    } else { // Should not happen.
        throw("allocator: free segment with no free pages");
    }

    segment.n_used_pages++;
    if segment.n_used_pages == segment.n_total_pages { // Segment is full.
        self.dequeueFreeSegment(segment);
    }

    // Prepare the page for allocation.
    page.block_size = pq.block_size;
    page.makeReady();

    self.enqueuePage(pq, page);
    return page;
}

func MHeap.allocHugePage(size: uint) *MPage {
    // Compute the actual size needed for allocation.
    let segment_size = @sizeof(MSegment) - (M_SMALL_PAGES_PER_SEGMENT - 1) * @sizeof(MPage);
    let full_size = sysAlignHugePageSize(size + segment_size);
    if full_size > M_MAX_ALLOC_SIZE {
        panic("requested allocation size is too large");
    }

    // Allocate the backing segment.
    let segment = mSegmentAlloc(self.thr_id, full_size, .Huge);

    // Allocate the first (and only) page of the segment.
    let page = &segment.pages[0];
    segment.n_used_pages++;

    page.block_size = mAlignUp(size, M_MIN_ALIGN);
    page.makeReady();

    self.enqueuePage(&self.pages[M_BIN_HUGE], page);
    return page;
}

/* --------------------------------- Freeing -------------------------------- */

func MHeap.free(data: *u8) {
    let block = unsafe(data as *MBlock);

    // Get the parent segment and page.
    let segment = mGetParentSegment(block);
    let page = segment.getParentPage(block);

    // Check if this a local or cross-thread free.
    if @atomic_load(&segment.xthr_id, AMO_RELAXED) == self.thr_id || self.tryReclaimOnFree(segment) {
        self.freeLocal(page, block);
    } else {
        self.freeCross(page, block);
    }
}

func MHeap.tryReclaimOnFree(segment: *MSegment) bool {
    // Try to claim the segment (only succeeds if abandoned).
    if !segment.tryClaim(self.thr_id) {
        return false;
    }

    // Load all the claimed pages into the appropriate page queues.
    for let i: uint = 0; i < segment.n_total_pages; i++ {
        let page = &segment.pages[i];
        let pq = self.getPageQueue(page);

        if page.n_used_blocks == 0 { // Page is free.
            continue;
        }

        // Collect all frees that happened between abandonment and reclamation.
        page.collectFree();
        if page.alloc_free == null { // Page is now free.
            // Manually free the page.
            page.flags = M_PAGE_RESET;
            page.n_used_blocks = 0;
            page.revival_chances = 0;

            sysReset(page.start, page.capacity);

            segment.n_used_pages--;
        } else { // Page has free blocks.
            self.enqueuePage(pq, page);
        }
    }

    // We should never be in a case where all the segment's pages are free since
    // we haven't yet performed the free which initiated the reclamation.
    // Hence, we put the segment into the appropriate free queue and carry on.
    if segment.n_used_pages < segment.n_total_pages {
        self.enqueueFreeSegment(segment);
    }

    return true;
}

func MHeap.freeLocal(page: *MPage, block: *MBlock) {
    // Add the block to the page's free list.
    block.next = page.local_free;
    page.local_free = block;
    page.n_used_blocks--;

    // Page is no longer zeroed.
    page.flags &= ~M_PAGE_ZEROED;

    if page.n_used_blocks == 0 { // Page is empty.
        self.freeOrRetirePage(page);
    } elif (page.flags & M_PAGE_FULL) > 0 { // Page is now unfull.
        self.removeFromFull(page);
    }
}

func MHeap.freeCross(page: *MPage, block: *MBlock) {
    let no_delay: bool;

    let tfree = @atomic_load(&page.xthr_free, AMO_RELAXED);
    let new_tfree: MThrFreeList;
    do {
        no_delay = tfree.getDelay() != MThrDelay.Delayed;
        if no_delay { // Fast Path
            // Push onto thread free list.
            block.next = tfree.getBlock();
            new_tfree = tfree.setBlock(block);
        } else { // Slow Path
            // Take ownership of delayed list.
            new_tfree = tfree.setDelay(.Delaying);
        }
    } while !@atomic_cas_weak(&page.xthr_free, &tfree, new_tfree, AMO_RELEASE, AMO_RELAXED);

    if no_delay { // Fast Path
        // Already pushed onto thread free list: done!
        return;
    }

    // Push onto delayed free list.
    let dfree = @atomic_load(&self.xdelayed_free, AMO_RELAXED);
    do {
        block.next = dfree;
    } while !@atomic_cas_weak(&self.xdelayed_free, &dfree, block, AMO_RELEASE, AMO_RELAXED);

    // Clear the delayed status on the thread free list.
    tfree = @atomic_load(&page.xthr_free, AMO_RELAXED);
    do {
        new_tfree = tfree.setDelay(.NoDelay);
    } while !@atomic_cas_weak(&page.xthr_free, &tfree, new_tfree, AMO_RELEASE, AMO_RELAXED);
}

func MHeap.tryFreeDelayed(block: *MBlock, force: bool) bool {
    // Get the segment and page.
    let segment = mGetParentSegment(block);
    let page = segment.getParentPage(block);

    // This step is a bit subtle.  The general idea is that since we are
    // potentially removing a page's last "representative" block from the
    // delayed list, we want to reinstate the "delayed" flag, return to default
    // behavior.  If we didn't do this, we could end up with a case where
    // cross-thread frees linger in the page's thread free list until it happens
    // to be scanned when we are searching for free pages.  In that case, a page
    // could sit allocated for a substantially long time without being purged
    // even if its space isn't really being used.  Resetting the delayed flag
    // here avoids that by forcing subsequent frees to fallback to the delayed
    // mode thereby ensuring the page always has a block in the delayed list if
    // it is experiencing cross-thread freeing.
    if !page.waitAndSetDelay(.Delayed, force) {
        // If there is too much contention on the list, then we can just fail to
        // free the block and leave it in the delayed list for later.  This way,
        // we don't miss any cross-thread frees but also aren't stuck waiting
        // forever to get to free our block.
        return false;
    }

    // Collect all the other non-local frees to ensure an update-to-date usage
    // count, so we can check whether the page can be freed.
    page.collectThreadFree();

    // Free the delayed block as a local block.
    self.freeLocal(page, block);
    return true;
}

func MHeap.freeOrRetirePage(page: *MPage) {
    // Get the page's associate page queue.
    let pq = self.getPageQueue(page);

    if pq.block_size == 0 { // Special page queue
        self.freePage(pq, page);
    } elif pq.first == page && pq.last == page { // Page queue contains only 1 page
        self.retirePage(pq, page);
    } else { // Page queue contains more than 1 page
        self.freePage(pq, page);
    }
}

func MHeap.freePage(pq: *MPageQ, page: *MPage) {
    // Remove the page from its parent page queue.
    self.dequeuePage(pq, page);

    // Make sure the page is properly cleared.
    page.flags = 0;
    page.n_used_blocks = 0;
    page.revival_chances = 0;

    // Update the parent segment's usage count.
    let segment = mGetParentSegment(unsafe(page as *MBlock));

    if segment.n_used_pages == segment.n_total_pages { // Segment was full.
        self.enqueueFreeSegment(segment);
    }
    segment.n_used_pages--;

    if segment.n_used_pages == 0 { // Segment is empty.
        self.freeSegment(segment);
    } else { // Segment is still in use.
        // Reset the page.
        sysReset(page.start, page.capacity);
        page.flags = M_PAGE_RESET;
    }
}

func MHeap.retirePage(pq: *MPageQ, page: *MPage) {
    // Mark the page as retired.
    page.flags |= M_PAGE_RETIRED;

    // Set its number of revival chances.
    if page.block_size > M_MAX_SMALL_OBJ_SIZE {
        page.revival_chances = M_REVIVAL_CHANCES;
    } else {
        page.revival_chances = M_REVIVAL_CHANCES_SMALL;
    }

    // Update the retirement indices.
    let ndx = self.getQueueIndex(pq);
    if ndx < self.retire_ndx_min {
        self.retire_ndx_min = ndx;
    }

    if ndx > self.retire_ndx_max {
        self.retire_ndx_max = ndx;
    }
}

/* ----------------------- Initialization and Cleanup ----------------------- */

// The empty page: used as a placeholder to avoid an extra conditional check
// when allocation from direct bins.
let _mpage_empty: MPage = null;

// The 72 size-classes represented in multiples of the machine word size.
const _mbin_word_sizes: []uint = [
    1,      2,      3,      4,      5,      6,      7,      8,       // Bins 0-7
    10,     12,     14,     16,     20,     24,     28,     32,      // Bins 8-15
    40,     48,     56,     64,     80,     96,     112,    128,     // Bins 16-23
    160,    192,    224,    256,    320,    384,    448,    512,     // Bins 24-31
    640,    768,    896,    1024,   1280,   1536,   1792,   2048,    // Bins 32-39
    2560,   3072,   3584,   4096,   5120,   6144,   7168,   8192,    // Bins 40-47
    10240,  12288,  14336,  16384,  20480,  24576,  28672,  32768,   // Bins 48-55
    40960,  49152,  57344,  65536,  81920,  98304,  114688, 131072,  // Bins 56-63
    163840, 196608, 229376, 262144, 372680, 393216, 458752, 524288,  // Bins 64-71
    0,  // Huge Bin
    0   // Full List
];

func mRoundToSizeClass(size: uint) uint {
    let bin = mGetPageBin(size);
    if bin >= M_BIN_HUGE {
        panic("allocator: heap size class is huge");
    }
    return _mbin_word_sizes[bin] * M_WORD_SIZE;
}

func MHeap.init(back_segment: *MSegment, back_page: *MPage) {
    // Set the heap's thread ID (held constant over its lifetime).
    self.thr_id = sysGetThrID();

    // Initialize the free segment queues.
    self.small_free_segments = null;
    self.med_free_segments = null;
    if back_segment.n_used_pages < back_segment.n_total_pages { // Backing segment has free space
        self.enqueueFreeSegment(back_segment);
    }

    // Initialize the direct bins and page queues as empty.
    for let i: uint = 0; i < M_N_DIRECT_BINS; i++ {
        self.direct_pages[i] = &_mpage_empty;
    }

    for let i: uint = 0; i < M_N_TOTAL_BINS; i++ {
        let pq = &self.pages[i];

        pq.block_size = _mbin_word_sizes[i] * M_WORD_SIZE;
        pq.first = null;
        pq.last = null;
    }

    // Initialize the delayed free list as empty.
    self.xdelayed_free = null;

    // Set the retirement indices to indicate no retired pages.
    self.retire_ndx_min = M_N_REG_BINS - 1;
    self.retire_ndx_max = 0;

    if back_page.alloc_free == null { // Back page is full.
        // Add the backing page to the full list.
        back_page.flags |= M_PAGE_FULL;
        self.enqueuePage(&self.pages[M_BIN_FULL], back_page);
    } else { // Back page has room.
        // Add the backing page to its page queue to be used for allocation.
        let pq = self.getPageQueue(back_page);
        self.enqueuePage(pq, back_page);
    }
}

func MHeap.cleanup() {
    // Free all pages that are empty.
    for let i: uint = 0; i < M_N_TOTAL_BINS; i++ {
        let pq = &self.pages[i];
        let page = pq.first;

        while page != null {
            let next = page.next;

            if page.n_used_blocks == 0 { // No live blocks: empty.
                self.freePage(pq, page);
            } else {
                // Disable delayed freeing: prepare for abandonment.
                page.waitAndSetDelay(.NeverDelay, true);
            }

            page = next;
        }
    }

    // Collect all the delayed free blocks.  After this collection, no more delayed
    // frees should be able to occur.  All pages have delayed freeing disabled.
    self.collectDelayed(true /* force */);

    // Get the backing segment and page of the heap.
    let back_segment = mGetParentSegment(unsafe(self as *MBlock));
    let back_page = back_segment.getParentPage(unsafe(self as *MBlock));

    // Abandon or free all remaining pages.
    for let i: uint = 0; i < M_N_TOTAL_BINS; i++ {
        let pq = &self.pages[i];
        let page = pq.first;

        while page != null {
            let next = page.next;

            page.collectFree();

            if page == back_page {
                // Backing page must be handled separately.
                continue;
            } elif page.n_used_blocks == 0 { // Delay frees emptied page.
                self.freePage(pq, page);
            } else { // Page has live blocks.
                page.abandon();
            }
        }
    }

    // Free the heap's backing memory CAREFULLY.
    if back_segment.n_used_pages == 1 { // Heap is only allocated block.
        back_segment.n_used_pages--;

        if back_segment.n_used_pages == 0 { // Backing page is only page in its segment.
            mSegmentFree(back_segment);
        } else {
            // Free the heap's backing page explicitly.  We don't need to reset it.
            back_page.n_used_blocks = 0;
            back_page.flags = 0;
            back_page.next = null;
            back_page.prev = null;
            
            // Abandon the backing segment.
            back_segment.abandon();
        }
    } else {
        unsafe {
            // Free the block containing the heap explicitly.
            let block = self as *MBlock;
            block.next = back_page.local_free; // Heap state is corrupted.
            back_page.local_free = block;
            back_page.n_used_blocks--;
        }

        // Abandon the backing page.
        back_page.abandon();
    }
}

/* ------------------------------ Page Queuing ------------------------------ */

// MPageQ is a queue of ready pages.
struct MPageQ {
    first, last: *MPage;
    block_size: uint;
}

func MHeap.moveToFull(src_pq: *MPageQ, page: *MPage) {
    // Remove the page from its old page queue.
    self.dequeuePage(src_pq, page);

    // Mark it as full.
    page.flags |= M_PAGE_FULL;

    // Add it to the full list.
    self.enqueuePage(&self.pages[M_BIN_FULL], page);
}

func MHeap.removeFromFull(page: *MPage) {
    // Remove the page from the full list.
    self.dequeuePage(&self.pages[M_BIN_FULL], page);

    // Remove its full flag.
    page.flags &= ~M_PAGE_FULL;

    // Add it to its destination queue.
    let dest_pq = self.getPageQueue(page);
    self.enqueuePage(dest_pq, page);
}

func MHeap.enqueuePage(pq: *MPageQ, page: *MPage) {
    // Link the page into the queue.
    page.next = null;
    page.prev = pq.last;
    pq.last = page;

    if pq.first == null { // Queue was empty
        // page is now also front of queue.
        pq.first = page;
        self.updateQueueFirst(pq);
    }
}

func MHeap.dequeuePage(pq: *MPageQ, page: *MPage) {
    // Remove page from the chain by updating prev and next.
    if page.next != null {
        page.next.prev = page.prev;
    }

    if page.prev != null {
        page.prev.next = page.next;
    }

    if pq.first == page { // page was the first element
        // First element is now page's next element.
        pq.first = page.next;
        self.updateQueueFirst(pq);
    }

    if pq.last == page { // page was the last element.
        // Last element is now page's previous element.
        pq.last = page.prev;
    }

    // Reset page's prev and next pointers.
    page.next = null;
    page.prev = null;
}

func MHeap.updateQueueFirst(pq: *MPageQ) {
    if pq.block_size == 0 || pq.block_size > M_MAX_DIRECT_SIZE { // Special or bigger than direct
        // No update needed.
        return;
    }

    // Get the new first page.
    let page = pq.first;
    if page == null {
        // If the first is empty, then we want the direct bins to point to the
        // empty page instead of just being a null pointer.  This avoids an
        // extra comparison on the fast path during allocation.
        page = &_mpage_empty;
    }

    // Check whether the direct bin has already been updated.
    let end = mGetWordSize(pq.block_size);
    if page == self.direct_pages[end] {
        return;
    }

    // Determine the start index of the range of bins that should be updated.
    let start: uint = 0;
    if end > 1 { // Not first bin
        // Work out the size of the previous valid bin.  On a some platforms,
        // certain bins are unused due to alignment considerations.  Hence, we
        // want to skip until we find the first page queue whose block size
        // corresponds to an actually usable bin.
        let bin = mGetPageBin(page.block_size);

        let prev = pq;
        unsafe {
            prev--;

            while prev > &self.pages[0] && bin == mGetPageBin(prev.block_size) {
                prev--;
            }
        }

        // The start index for pq's direct bins will be the word size of the
        // previous usable bin + 1 because the previous bin will be used for
        // direct bins with size <= prev.block_size.
        start = 1 + mGetWordSize(prev.block_size);

        // If the start index is too large (due to rounding), just set it equal
        // to the end index: we will be replacing only one direct bin.
        if start > end {
            start = end;
        }
    }

    // Replace all associated direct bins with the new page.
    for let i = start; i <= end; i++ {
        self.direct_pages[i] = page;
    }
}

func MHeap.getPageQueue(page: *MPage) *MPageQ {
    if (page.flags & M_PAGE_FULL) > 0 { // Full list
        return &self.pages[M_BIN_FULL];
    }

    let bin = mGetPageBin(page.block_size);
    return &self.pages[bin];
}

@inline
func MHeap.getQueueIndex(pq: *MPageQ) uint {
    unsafe {
        return pq - &self.pages[0] as uint;
    }
}

func mGetPageBin(size: uint) uint {
    const MAX_LARGE_OBJ_WSIZE = M_MAX_LARGE_OBJ_SIZE / M_WORD_SIZE;
    const ALIGN2W = M_MIN_WORD_ALIGN == 2;
    const ALIGN4W = M_MIN_WORD_ALIGN == 4;

    let wsize = mGetWordSize(size);

    if wsize <= 1 { // Smallest Bin
        return 0;  
    } elif wsize >= MAX_LARGE_OBJ_WSIZE { // Huge Bin
        return M_BIN_HUGE;
    }

    if ALIGN2W && wsize <= 8 { // Min word align = 2
        // Round to a multiple of 2.
        wsize = (wsize + 1) & ~1;
    } elif ALIGN4W && wsize <= 16 { // Min word align = 4
        // Round to a multiple of 4.
        wsize = (wsize + 3) & ~3;
    }

    if wsize <= 8 { // Tiny Bin
        return wsize - 1;
    }

    // The leading 3 bits of (wsize - 1) uniquely identify the bin.  The MSB by
    // definition is always 1, but it's position identifies a group of four
    // possible bins.  The second and third most-significant bits then identify
    // one of the four bins. For example, consider bins 8-11:
    //
    //   Bin | Size | MSB of (size - 1) | 23 
    //   -----------------------------------
    //     8 |   10 |                 3 |  0
    //     9 |   12 |                 3 |  1
    //    10 |   14 |                 3 |  2
    //    11 |   16 |                 3 |  3
    //
    // Hence, given a word of size, say 12, we can map it to its correct bin by: 
    // 
    //   (MSB << 2) + ((wsize >> (MSB - 2)) & 3) - 4 = 12 + 1 - 4 = 9 
    // 
    // This formula will also work for word sizes between bins due to how the
    // bins are spaced.
    wsize--;

    // Find the index of the most significant bit (bit scan reverse).
    let msb: uint = 0;
    while (1 << msb) < wsize {
        msb++;
    }
    
    return (msb << 2) + ((wsize >> (msb - 2)) & 3) - 4;


}

@inline
func mGetWordSize(size: uint) uint {
    return (size + M_WORD_SIZE - 1) / M_WORD_SIZE;
}

/* ---------------------------------- Pages --------------------------------- */

// MThrFreeList is the page-local cross-thread free list (thread-free).  This is
// used as an optimization to reduce contention in the delayed free list.  The
// lower 2 bits of the uint are used to store the delay state and the remaining
// bits are a pointer to the front of the list.
type MThrFreeList = uint;

// MThrDelay enumerates the possible delay states of the thread-free list.
enum MThrDelay {
    Delayed = 0;
    Delaying = 1;
    NoDelay = 2;
    NeverDelay = 3;
}

func MThrFreeList.getBlock() *MBlock {
    unsafe {
        return (*self & ~3) as *MBlock;
    }
}

func MThrFreeList.setBlock(block: *MBlock) MThrFreeList {
    unsafe {
        return (*self & 3) | (block as uint);
    }
}

func MThrFreeList.getDelay() MThrDelay {
    unsafe {
        return (*self & 3) as MThrDelay;
    }
}

func MThrFreeList.setDelay(delay: MThrDelay) MThrFreeList {
    return (*self & ~3) | (delay as uint);
}

// MPage is a collection of memory blocks which are all of the same size and
// belong to the same thread.  Pages are used to realize the sharded free-list
// design of the allocation.
struct MPage {
    // Page specific flags (uses the M_PAGE_* values).
    flags: u32;

    // The number of revival chances the page has left during retirement.  If
    // the page is not retired, this field is unused (should be 0).
    revival_chances: u32;

    // Pages are organized into doubly-linked lists called page queues.  The
    // entries in the page queues are pages.  These pointers point to the
    // previous and next pages in the page's parent page queue.
    prev, next: *MPage;

    // The number of committed and reserved bytes backing the page.
    capacity, reserved: uint;

    // The byte size of blocks (size class) that can be allocated in the page.
    block_size: uint;
    
    // The number of block currently allocated.
    n_used_blocks: uint;

    // The allocation and local free lists.
    alloc_free, local_free: *MBlock;

    // The thread free list.  Must be accessed atomically.
    xthr_free: MThrFreeList;

    // The start address of the page's memory.
    start: *u8;
}

// MBlock represents a free block of memory in a page's free list.
// Conceptually, the block is to be viewed as a chunk of memory of size
// block_size (stored in its parent page) where the first word-size bytes are
// used to store the pointer to the next block in the list:
//
//    | next, ... | --> | next, ... | --> ...
// 
struct MBlock {
    next: *MBlock;
}

func MPage.zeroInit(page_start: *u8) {
    self.flags = M_PAGE_ZEROED;
    self.revival_chances = 0;

    self.block_size = 0;

    self.prev = null;
    self.next = null;

    self.n_used_blocks = 0;
    self.capacity = 0;
    self.reserved = 0;

    self.alloc_free = null;
    self.local_free = null;
    self.xthr_free = 0;

    self.start = page_start;
}

func MPage.makeReady() {
    // Check for empty page.
    if self.reserved == 0 || self.block_size == 0 {
        throw("allocator: cannot make empty page ready");
    }

    if self.capacity == 0 { // Page has reserved space that is not committed.
        sysCommit(self.start, self.reserved);
        self.capacity = self.reserved;
    }

    if (self.flags & M_PAGE_RESET) > 0 { // Page has been reset.
        sysUnreset(self.start, self.capacity);
        self.flags &= ~M_PAGE_RESET;
    }

    // Initialize the allocation free list.
    unsafe {
        let curr = self.start as *MBlock;
        let prev: *MBlock = null;

        let n_blocks = self.capacity / self.block_size;
        for let i: uint = 0; i < n_blocks; i++ {
            curr.next = prev;
            prev = curr;
            curr = (curr as *u8) + self.block_size as *MBlock;
        }

        self.alloc_free = prev;
    }

    // Local and thread free lists start as empty.
    self.local_free = null;
    self.xthr_free = 0;
}

func MPage.popFreeBlock() *u8 {
    // Pop a block from the front of the allocation list.
    let block = self.alloc_free;
    self.alloc_free = block.next;
    self.n_used_blocks++;

    let data = unsafe(block as *u8);

    // Zero the block.
    if (self.flags & M_PAGE_ZEROED) > 0 {
        block.next = null;
    } else {
        memset(data, 0, self.block_size);
    }

    return data;
}

func MPage.collectFree() {
    // Collect thread free list.
    self.collectThreadFree();

    // Collect local free list.
    if self.alloc_free == null {
        self.alloc_free = self.local_free;
        self.local_free = null;
    }
}

func MPage.collectThreadFree() {
    // Atomically acquire the thread list.
    let tfree = @atomic_load(&self.xthr_free, AMO_RELAXED);
    let new_tfree: MThrFreeList = 0;
    let head: *MBlock = null;
    do {
        head = tfree.getBlock();
        new_tfree = tfree.setBlock(null);
    } while !@atomic_cas_weak(&self.xthr_free, &tfree, new_tfree, AMO_ACQ_REL, AMO_RELAXED);

    // Thread free list is empty: nothing to do.
    if head == null {
        return;
    }

    // Find the tail of the thread free list and update the usage count.
    let tail = head;
    self.n_used_blocks--;
    while tail.next != null {
        tail = tail.next;
        self.n_used_blocks--;
    }

    // Concatenate the thread free list onto the front of the local list.
    tail.next = self.local_free;
    self.local_free = head;
}

func MPage.waitAndSetDelay(delay: MThrDelay, force: bool) bool {
    let attempts = 0;

    let tfree: MThrFreeList;
    let new_tfree: MThrFreeList;
    let old_delay: MThrDelay;

    do {
        // Load with acquire since we may repeat/exit loop without doing a CAS.
        tfree = @atomic_load(&self.xthr_free, AMO_ACQUIRE);
        old_delay = tfree.getDelay();

        if old_delay == MThrDelay.Delaying { // Other thread is working on list: can't change delay.
            // If not forced, give up after 4 tries.
            if !force && attempts > 4 { // Too much contention.
                return false;
            }
            attempts++;

            // Yield and try again.
            thrYield();
            continue;
        } elif old_delay == MThrDelay.NeverDelay {
            // Never delay can't be overwritten.
            return false;
        } elif old_delay == delay {
            // No more work to be done: delay is already correct.
            return true;
        }
        
        // Update delay.
        new_tfree = tfree.setDelay(delay);
    } while !@atomic_cas_weak(&self.xthr_free, &tfree, new_tfree, AMO_RELEASE, AMO_RELAXED);

    return true;
}

func MPage.enableDelayedFree() {
    let tfree = @atomic_load(&self.xthr_free, AMO_RELAXED);
    let new_tfree: MThrFreeList;

    do {
        new_tfree = tfree.setDelay(.Delayed);
    } while !@atomic_cas_weak(&self.xthr_free, &tfree, new_tfree, AMO_RELEASE, AMO_RELAXED);
}

func MPage.abandon() {
    let segment = mGetParentSegment(unsafe(self as *MBlock));
    segment.n_abandoned_pages++;

    if segment.n_abandoned_pages == segment.n_used_pages { // Segment contains all abandoned pages.
        segment.abandon();
    }
}

/* ---------------------------- Segment Queueing ---------------------------- */

// MPageKind enumerates the different kinds of pages berry-malloc supports.
// Each generic category of page accomodates many different size classes, so
// freed pages can be reused for blocks of a different size class.
enum MPageKind {
    Small;
    Med;
    Large;
    Huge;
}

// MSegment is a backing-slab of memory which stores pages of a specific kind.
// Segments are always aligned on 4MiB boundaries (on 64-bit systems). For
// small, medium, and large pages, segments are 4MiB.  For huge pages (block
// size >= 2 MiB) segments are large enough to accomodate the full size of the
// allocation.  Conceptually, this struct represents the segment header after
// which the main content of the segment (data stored in pages) is placed.
struct MSegment {
    // The parent thread ID.  Must be accessed atomically (to support reclamation).
    xthr_id: uint;

    // The base address of the segment's backing memory.  This may not always be
    // the value of the segment pointer due to alignment offsets.
    base_addr: *u8;
    
    // The full size of the segment including alignment offset in bytes.
    full_size: uint;

    // The pointers the previous and next segment in the segment's free queue.
    prev, next: *MSegment;

    // The kind of pages stored in the segment.
    page_kind: MPageKind;

    // The page shift (size = 1 << shift) of the page's stored in the segment.
    page_shift: uint;

    // The number of allocated (used) and total pages in the segment.
    n_used_pages, n_total_pages: uint;

    // The number of abandoned pages in the segment.
    n_abandoned_pages: uint;

    // The pages stored in the segment.  Maximum # of pages = 64, but could be
    // less.  We only actually allocate the space we need, so bounds-checking on
    // this array is really more of a sanity check than anything else.
    pages: [M_SMALL_PAGES_PER_SEGMENT]MPage;

    // The size of the segment header.
    header_size: uint;
}

// MSegmentQ is a queue of free segments.
struct MSegmentQ {
    first, last: *MSegment;
}

func MHeap.findOrAllocFreeSegment(block_size: uint) *MSegment {
    let page_kind = mGetPageKind(block_size);

    // Try to find a segment with free pages.
    let segment: *MSegment;
    match page_kind {
    case .Small:
        segment = self.small_free_segments.findFreeSegment();
        if segment != null {
            return segment;
        }
    case .Med:
        segment = self.med_free_segments.findFreeSegment();
        if segment != null {
            return segment;
        }
    }

    // No free segments: allocate from OS.
    return self.allocRegularSegment(page_kind);
}

func MHeap.allocRegularSegment(page_kind: MPageKind) *MSegment {
    // Allocate a segment from the OS.
    let segment = mSegmentAlloc(self.thr_id, M_SEGMENT_SIZE, page_kind);
    if segment == null { // Segment allocation failed.
        return null;
    }

    // Add the segment to the appropriate free queue.
    self.enqueueFreeSegment(segment);
    return segment;
}

func MHeap.freeSegment(segment: *MSegment) {
    // Remove the segment from its free queue.
    self.dequeueFreeSegment(segment);

    // Return the segment to the OS.
    mSegmentFree(segment);
}

func MHeap.enqueueFreeSegment(segment: *MSegment) {
    match segment.page_kind {
    case .Small:
        self.small_free_segments.enqueue(segment);
    case .Med:
        self.med_free_segments.enqueue(segment);
    }
}

func MHeap.dequeueFreeSegment(segment: *MSegment) {
    match segment.page_kind {
    case .Small:
        self.small_free_segments.dequeue(segment);
    case .Med:
        self.med_free_segments.dequeue(segment);
    }
}

func MSegmentQ.enqueue(segment: *MSegment) {
    // Link the segment into the segment queue.
    segment.next = null;
    segment.prev = self.last;
    self.last = segment;

    if self.first == null { // Queue is empty.
        // segment becomes the front of the queue.
        self.first = segment;
    }
}

func MSegmentQ.dequeue(segment: *MSegment) {
    // Update the prev and next segments in the queue.
    if segment.next != null {
        segment.next.prev = segment.prev;
    }

    if segment.prev != null {
        segment.prev.next = segment.next;
    }

    // Remove the segment from the first and last of the queue.
    if self.first == segment {
        self.first = segment.next;
    }

    if self.last == segment {
        self.last = segment.prev;
    }

    // Reset segment next and prev pointers.
    segment.next = null;
    segment.prev = null;
}

func MSegmentQ.findFreeSegment() *MSegment {
    for let segment = self.first; segment != null; segment = segment.next {
        if segment.n_used_pages < segment.n_total_pages {
            return segment;
        }
    }

    return null;
}

/* -------------------------------- Segments -------------------------------- */

func mSegmentAlloc(thr_id: uint, segment_size: uint, page_kind: MPageKind) *MSegment {
    // Determine page information.
    let n_pages: uint = 1;
    let page_shift: uint = 0;

    match page_kind {
    case .Small:
        n_pages = M_SMALL_PAGES_PER_SEGMENT;
        page_shift = M_SMALL_PAGE_SHIFT;
    case .Med:
        n_pages = M_MED_PAGES_PER_SEGMENT;
        page_shift = M_MED_PAGE_SHIFT;
    case .Large:
        n_pages = M_LARGE_PAGES_PER_SEGMENT;
        page_shift = M_LARGE_PAGE_SHIFT;
    }

    let page_size = 1 << page_shift;
    if page_kind == MPageKind.Huge {
        page_size = segment_size;
    }

    // Reserve (or allocate) the space for the segment.
    let full_commit = !sys_mem_info.can_reserve;
    let alloc_info = mOSAllocAligned(segment_size, M_SEGMENT_ALIGN, full_commit);
    if alloc_info.ptr == null { // Out of memory
        return null;
    }

    // Commit the first page of memory.
    let pseg = alloc_info.ptr;
    if !full_commit {
        sysCommit(pseg, page_size);
    }

    // Initialize the segment info.
    let segment = unsafe(pseg as *MSegment);
    segment.base_addr = alloc_info.base;
    segment.full_size = alloc_info.full_size;

    segment.xthr_id = thr_id;
    segment.page_kind = page_kind;
    segment.page_shift = page_shift;
    segment.n_used_pages = 0;
    segment.n_total_pages = n_pages;
    segment.n_abandoned_pages = 0;
    segment.header_size = @sizeof(MSegment) - (M_SMALL_PAGES_PER_SEGMENT - n_pages) * @sizeof(MPage);

    // Initialize the first page.
    let page = &segment.pages[0];

    let first_page_start = unsafe((segment as *u8) + segment.header_size);
    page.zeroInit(first_page_start);
    page.reserved = page_size - segment.header_size;
    page.capacity = page.reserved;

    // Initialize the remaining pages.
    for let i: uint = 1; i < n_pages; i++ {
        let page_start = unsafe((segment as *u8) + i * page_size);

        page = &segment.pages[i];
        page.zeroInit(page_start);
        page.reserved = page_size;

        if full_commit {
            page.capacity = page.reserved;
        }
    }

    return segment;
}

struct OSAllocInfo {
    ptr: *u8;
    base: *u8;
    full_size: uint;
}

func mOSAllocAligned(size, align: uint, commit: bool) OSAllocInfo {
    size = mAlignUp(size, sys_mem_info.os_page_size);

    // Try to allocate using align as a hint.
    let p = sysAlloc(null, size, align, commit);
    if p == null {
        return null;
    }

    unsafe {
        if (p as uint) % align == 0 { // Already aligned
            return .{ p, p, size };
        }
    }

    let base: *u8;
    let full_size: uint;
    if sys_mem_info.can_partial_free { // mmap can free inside allocated blocks.
        // TODO
    } else { // VirtualAlloc can't.
        // Over-allocate to accomodate alignment.  We always reserve here to
        // avoid wasting physical frames on alignment.
        full_size = size + align;
        p = sysAlloc(null, full_size, 0, false);
        if p == null {
            return null;
        }

        // Save the base pointer into the segment's base address.
        base = p;

        // Align the pointer to the right region.
        p = mAlignUpPtr(p, align);

        // Commit only the aligned part as necessary.
        if commit {
            sysCommit(p, size);
        }
    }

    return .{p, base, full_size};
}

func mSegmentFree(segment: *MSegment) {
    sysRelease(segment.base_addr, segment.full_size);
}

func MSegment.tryClaim(thr_id: uint) bool {
    // Atomically test if the segment has been abandoned.
    let old_id: uint = 0;
    do {
        // Use ACQUIRE here since loop may exit w/o a CAS.
        old_id = @atomic_load(&self.xthr_id, AMO_ACQUIRE);

        if old_id != 0 { // Segment has been claimed: exit.
            return false;
        }
    } while !@atomic_cas_weak(&self.xthr_id, &old_id, thr_id, AMO_RELEASE, AMO_RELAXED);

    // Reset the number of abandoned pages (all segment's pages are reclaimed).
    self.n_abandoned_pages = 0;

    // Enable delayed freeing on all abandoned pages.
    for let i: uint = 0; i < self.n_total_pages; i++ {
        let page = &self.pages[i];

        if page.n_used_blocks > 0 {
            page.enableDelayedFree();
        }
    }

    return true;
}

@inline
func MSegment.abandon() {
    // Clear the owning thread ID: abandon the segment.
    @atomic_store(&self.xthr_id, 0, AMO_RELEASE);
}

func MSegment.getParentPage(block: *MBlock) *MPage {
    if self.page_kind == MPageKind.Huge {
        return &self.pages[0];
    }

    unsafe {
        let seg_offset = (block as *u8) - (self as *u8) as uint;
        let page_ndx = seg_offset >> self.page_shift;

        if page_ndx >= self.n_total_pages {
            throw("allocator: page lookup in segment is out of bounds");
        }

        return &self.pages[page_ndx];
    }
}

func mGetParentSegment(block: *MBlock) *MSegment {
    unsafe {
        return (block as uint) & ~M_SEGMENT_MASK as *MSegment;
    }
}

func mGetPageKind(block_size: uint) MPageKind {
    if block_size <= M_SMALL_PAGE_SIZE {
        return .Small;
    } elif block_size <= M_MED_PAGE_SIZE {
        return .Med;
    } elif block_size <= M_LARGE_PAGE_SIZE {
        return .Large;
    } 

    return .Huge;
}

@inline
func mAlignUp(size, align: uint) uint {
    // NOTE: Align must be a power of 2.
    return (size + align - 1) & ~(align - 1);
}

@inline
func mAlignUpPtr(ptr: *u8, align: uint) *u8 {
    unsafe {
        return mAlignUp(ptr as uint, align) as *u8;
    }
}