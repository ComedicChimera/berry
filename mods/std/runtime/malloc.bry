module runtime;

/* ------------------------------ Allocator API ----------------------------- */

pub func malloc(size: uint) *u8 {
    let rstate = get_rstate();

    rstate.panic_depth++;
    let data = rstate.heap.malloc(size);
    rstate.panic_depth--;

    return data;
}

pub func mrealloc(data: *u8, new_size: uint) *u8 {
    let rstate = get_rstate();

    rstate.panic_depth++;
    data = rstate.heap.realloc(data, new_size);
    rstate.panic_depth--;

    return data;
}

pub func mfree(data: *u8) {
    let rstate = get_rstate();

    rstate.panic_depth++;
    rstate.heap.mfree(data);
    rstate.panic_depth--;
}

/* -------------------------------- Constants ------------------------------- */

// Useful Size Constants
const KB: uint = 1024;
const MB: uint = KB * KB;
const GB: uint = KB * KB * KB;

// Platform Machine Word Size
const WORD_SHIFT: uint = #if (ARCH_BITS == 64) 3 #else 2 #end;
const WORD_SIZE: uint = 1 << WORD_SHIFT;

// Page Sizing
const SMALL_PAGE_SHIFT: uint = 13 + WORD_SHIFT;
const SMALL_PAGE_SIZE: uint = 1 << SMALL_PAGE_SHIFT;  // 64 KiB

const MED_PAGE_SHIFT: uint = 3 + SMALL_PAGE_SHIFT;
const MED_PAGE_SIZE: uint = 1 << MED_PAGE_SHIFT;  // 512 KiB

const LARGE_PAGE_SHIFT: uint = 3 + MED_PAGE_SHIFT;
const LARGE_PAGE_SIZE: uint = 1 << LARGE_PAGE_SHIFT;  // 4 MiB

// Segment Sizing
const SEGMENT_SHIFT: uint = LARGE_PAGE_SHIFT;
const SEGMENT_SIZE: uint = 1 << SEGMENT_SHIFT;
const SEGMENT_ALIGN: uint = SEGMENT_SIZE;
const SEGMENT_MASK: uint = SEGMENT_ALIGN - 1;

const SMALL_PAGES_PER_SEGMENT: uint = SEGMENT_SIZE / SMALL_PAGE_SIZE;
const MED_PAGES_PER_SEGMENT: uint = SEGMENT_SIZE / MED_PAGE_SIZE;
const LARGE_PAGES_PER_SEGMENT: uint = SEGMENT_SIZE / LARGE_PAGE_SIZE;

// Allocation Size Limits
const MAX_SMALL_SIZE: uint = SMALL_PAGE_SIZE / 4;    // 16 KiB
const MAX_MED_SIZE: uint = MED_PAGE_SIZE / 4;        // 128 KiB
const MAX_LARGE_SIZE: uint = LARGE_PAGE_SIZE / 2;    // 2 MiB

const MAX_ALLOC_SIZE: uint = 1 << (@sizeof(*u8) - 1);

// Bin Counts
const N_DIRECT_BINS: uint = KB / WORD_SIZE;
const N_REGULAR_BINS: uint = 73;
const BIN_FULL: uint = N_REGULAR_BINS;
const BIN_HUGE: uint = N_REGULAR_BINS + 1;
const N_TOTAL_BINS: uint = N_REGULAR_BINS + 2;

// Page Flags
const PAGE_ZEROED: byte = 1;
const PAGE_FULL: byte = 2;
const PAGE_RETIRED: byte = 4;

// Retirement
const RETIRE_COUNTER_MAX: byte = 16;

/* ------------------------ Allocator Data Structures ----------------------- */

struct MHeap {
    thread_id: int;
    
    direct_bins: [N_DIRECT_BINS]*MPage;
    pages: [N_TOTAL_BINS]MPageQ;

    small_segments: MSegmentQ;
    med_segments: MSegmentQ;

    delayed_free: *MBlock;
}

struct MSegment {
    thread_id: int;
    size: uint;

    prev, next: *MSegment;

    page_kind: MPageKind;
    page_shift: uint;

    n_used_pages: uint;
    n_total_pages: uint;

    // NOTE: Small pages per segment is the maximum number of pages a segment
    // can have. In reality, there may be fewer than this number of pages
    // actually allocated in the header.
    pages: [SMALL_PAGES_PER_SEGMENT]MPage;

    header_size: uint;
}

struct MSegmentQ {
    first, last: *MSegment;
}

enum MPageKind {
    Small,
    Med,
    Large,
    Huge
}

type MThreadFreeList = uint;

enum MThreadFreeMode {
    Normal,
    Delayed
}

@inline
func MThreadFreeList.get_block() *MBlock {
    unsafe {
        return (*self & ~0b11) as *MBLock;
    }
}

@inline
func MThreadFreeList.get_mode() MThreadFreeMode {
    unsafe {
        return (*self & 0b11) as MThreadFreeMode;
    }
}

@inline
func MThreadFreeList.set_block(block: *MBlock) MThreadFreeList {
    unsafe {
        return (block as MThreadFreeList) | (self.get_mode() as MThreadFreeList);
    }
}

@inline
func MThreadFreeList.set_mode(mode: MThreadFreeMode) MThreadFreeList {
    unsafe {
        return (mode as MThreadFreeList) | (self.get_block() as MThreadFreeList);
    }
}

struct MPage {
    next, prev: *MPage;

    used, cap, reserved: uint;

    block_size: uint;
    alloc_free: *MBlock;
    local_free: *MBlock;
    thread_free: MThreadFreeList;

    page_start: *u8;

    flags: byte;
    retire_counter: byte;
}

struct MPageQ {
    first, last: *MPage;
    block_size: uint;
}

struct MBlock {
    next: *MBlock;
}

/* --------------------------- Malloc and Realloc --------------------------- */

func MHeap.malloc(size: uint) *u8 {
    // Find a free page to allocate in.
    if size <= KB { // Fast Path
        let direct_bin = get_word_size(size);
        let page = self.direct_bins[direct_bin];

        if page.alloc_free == null { // Fast Path failed
            // Fallback to generic malloc.
            let new_page = self.find_free_page(size);

            // Collection didn't free up current page.
            if page.alloc_free == null {
                // Mark direct bin as full and replace it.
                let bin = get_page_bin(page.block_size);
                self.make_full(&self.pages[bin], page);
                self.replace_direct(direct_bin, new_page);
            }   

            page = new_page;
        }
    } else { // Slow Path
        page = self.find_free_page(size);
    }

    // `find_free_page` fails internally if it runs out of memory, so we don't
    // need to check if page is valid here.  Thus, we can just directly allocate
    // from the `page` we've found.
    return page.pop_free_block();
}

func MHeap.mrealloc(data: *u8, new_size: uint) *u8 {
    // TODO
}

func MHeap.find_free_page(size: uint) {
    let bin = get_page_bin(size);
    let pageq = &self.pages[bin];

    let page = pageq.first;
    while page != null {
        // Try to collect the page's free lists.
        page.collect_free();

        if page.alloc_free != null { // Found a free page!
            return page;
        }

        // Mark the page as full and move on.
        let next = page.next;
        self.make_full(pageq, page);
        page = next;
    }

    // Otherwise, allocate a fresh page.
    return self.page_alloc(pageq, size);
}

func MHeap.page_alloc(pageq: *MPageQ) {
    // Find a segment with a available pages.
    let page_kind = get_page_kind(pageq.block_size);
    let segment = self.find_free_segment(page_kind);

    // Grab a free page from the segment.
    let page: *MPage;
    for let i = 0; i < segment.n_total_pages; i++ {
        page = &segment.pages[i];

        if page.used == 0 {
            break;
        }
    } else { // No free pages: should never happen.
        throw("free segment with no free pages")
    }

    // Commit the page as necessary.
    if page.capacity == 0 {
        sys_commit(page.page_start, page.reserved, 0);
        page.capacity = page.reserved;
    }

    // Reinitialize the page.
    page.zero_init();
    page.block_size = pageq.block_size;
    page.init_freelists();

    // Return the freshly allocated page.
    return page;
}

func MHeap.mark_full(pageq: *MPageQ, page: *MPage) {
    // TODO: set thread_free state (handle potential race conditions)

    // Dequeue the page from its current page queue.
    pageq.dequeue(page);

    // Add it to the full queue.
    self.pages[BIN_FULL].enqueue(page);

    // Update its flags.
    page.flags |= PAGE_FULL;
}

func MHeap.replace_direct(direct_bin: uint, new_page: *MPage) {
    // TODO
}

/* ---------------------------------- Free ---------------------------------- */

func MHeap.mfree(data: *u8) {
    // TODO
}

/* ----------------------------- Initialization ----------------------------- */

func MHeap.init(segment: *MSegment) {
    // TODO   
}

/* ----------------------------- Page Management ---------------------------- */

func MPage.pop_free_block() *u8 {
    let block = page.alloc_free;
    page.alloc_free = block.next;
    page.used += page.block_size;

    // Zero out the new memory block.
    if (page.flags & PAGE_ZEROED) > 0 {
        block.next = null;
    } else {
        unsafe {
            memset(block as *u8, 0, page.block_size);
        }
    }

    // Return the allocated memory.
    unsafe {
        return block as *u8;
    }
}

func MPage.zero_init() {
    // TODO
}

func MPage.init_freelists() {
    // TODO
}

func MPage.collect_free() {
    // TODO
}

func MPageQ.enqueue(page: *MPage) {
    page.next = null;

    page.prev = self.last;
    self.last = page;

    if self.first == null {
        self.first = page;
    }
}

func MPageQ.dequeue(page: *MPage) {
    if page.next != null {
        page.next.prev = page.prev;
    }

    if page.prev != null {
        page.prev.next = page.next;
    }

    if self.first == page {
        self.first = page.next;
    }

    if self.last == page {
        self.last = page.prev;
    }
}

/* -------------------------------- Segments -------------------------------- */

func MHeap.find_free_segment(page_kind: MPageKind) *MSegment {
    // TODO
}

func msegment_sys_alloc(page_kind: MPageKind) *MSegment {
    // TODO
}

/* -------------------------------------------------------------------------- */

func get_page_kind(size: uint) MPageKind {
    if size <= MAX_SMALL_SIZE {
        return .Small;
    } elif size <= MAX_MED_SIZE {
        return .Med;
    } elif size <= MAX_LARGE_SIZE {
        return .Large;
    } elif size <= MAX_ALLOC_SIZE {
        return .Huge;
    } else {
        throw("allocation too large");
    }
}

func get_page_bin(size: uint) uint {
    // TODO
}

@inline
func get_word_size(size: uint) uint {
    return (size + 1) / WORD_SIZE;
}